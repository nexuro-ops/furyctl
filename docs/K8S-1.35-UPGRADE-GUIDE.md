# Kubernetes 1.35 Upgrade Guide

## Overview

This guide provides detailed step-by-step instructions for upgrading your SIGHUP Distribution clusters from Kubernetes 1.34 to 1.35.x. The upgrade path is straightforward but requires careful planning and testing.

## Prerequisites

### Pre-Upgrade Requirements

Before starting the upgrade, ensure you have:

- [ ] Kubernetes 1.34.x cluster currently running
- [ ] At least 30% free disk space on all nodes
- [ ] All modules at minimum compatible versions (see compatibility matrix)
- [ ] Recent backup of etcd
- [ ] Recent backup of persistent volumes
- [ ] Access to kubeconfig with admin privileges
- [ ] furyctl v0.34.0 or newer
- [ ] All workloads tested with 1.35 compatibility

### Environment Checks

```bash
# Check current Kubernetes version
kubectl version --short
# Expected output: Client and Server versions with v1.34.x

# Check node status
kubectl get nodes -o wide
# All nodes should be in Ready state

# Check for pending pods
kubectl get pods -A --field-selector=status.phase=Pending
# Should return no pending pods

# Check PVC status
kubectl get pvc -A
# All PVCs should be Bound

# Backup etcd (OnPremises only)
sudo cp -r /var/lib/etcd /var/lib/etcd.backup.1.34
```

### Breaking Changes Review

Review the breaking changes documentation before upgrading:

```bash
# Check your current module versions
furyctl get supported-versions --kind OnPremises # or EKS/KFDDistribution

# Compare with required versions for 1.35
cat docs/COMPATIBILITY-MATRIX.md | grep "1.35"

# Review specific breaking changes
cat docs/K8S-1.35-BREAKING-CHANGES.md
```

---

## Upgrade Procedures by Cluster Type

### OnPremises Clusters

#### Phase 1: System Preparation (Pre-Upgrade)

**1.1 Verify System Requirements**

```bash
# On each master and worker node, verify cgroup v2
stat -fc %T /sys/fs/cgroup/
# Expected output: cgroup2fs

# If output is tmpfs, you have cgroup v1 - MUST upgrade OS
# See remediation section below
```

**1.2 Verify containerd Version**

```bash
# On each node, check containerd version
containerd --version
# Expected output: containerd v2.0.x or later

# If older version, upgrade containerd:
# For Ubuntu/Debian:
sudo apt-get update
sudo apt-get install -y containerd.io=2.0.0 # or latest

# For RHEL/CentOS:
sudo yum update -y containerd
```

**1.3 Update furyctl Configuration**

```yaml
# Update your furyctl.yaml configuration
apiVersion: kfd.sighup.io/v1alpha2
kind: OnPremises
metadata:
  name: my-cluster
spec:
  distributionVersion: v1.35.0  # Change from v1.34.0

  # Rest of configuration remains the same
  hosts:
    - name: master-1
      ip: 10.0.0.10
      roles: [master, worker]
      # ... rest of node config
```

**1.4 Run Preflight Checks**

```bash
# Run furyctl preflight validations
furyctl create --config furyctl.yaml --dry-run

# Expected checks:
# - cgroup v2 verification ✓
# - containerd 2.0+ check ✓
# - kube-proxy mode check ✓
# - Node OS compatibility ✓
# - Module breaking changes ✓

# If any checks fail, resolve issues before continuing
```

**1.5 Backup Cluster State**

```bash
# Backup etcd
sudo systemctl stop etcd || true
sudo cp -r /var/lib/etcd /var/lib/etcd.backup.1.35
sudo systemctl start etcd || true

# Backup kubeconfig
cp ~/.kube/config ~/.kube/config.backup.1.34

# Backup all cluster state
kubectl get all -A -o yaml > cluster-state.1.34.yaml

# Backup CRDs
kubectl get crds -o yaml > crds.1.34.yaml
```

#### Phase 2: Cluster Upgrade

**2.1 Update Kubernetes Master**

```bash
# On primary master node
sudo kubeadm upgrade plan
# Review upgrade path - should show v1.34 -> v1.35

# Execute upgrade
sudo kubeadm upgrade apply v1.35.0

# Expected output: Upgrade successful, etcd upgraded
```

**2.2 Update Worker Nodes (One at a Time)**

```bash
# For each worker node:
WORKER_NAME="worker-1"

# Drain node
kubectl drain $WORKER_NAME \
  --ignore-daemonsets \
  --delete-emptydir-data

# Update kubelet on node
ssh user@$WORKER_NODE "sudo kubeadm upgrade node"

# Uncordon node
kubectl uncordon $WORKER_NAME

# Wait for node to be ready
kubectl wait --for=condition=Ready node/$WORKER_NAME --timeout=5m
```

**2.3 Verify Upgrade Success**

```bash
# Check Kubernetes version
kubectl version --short
# Should show: v1.35.0 or later

# Check all nodes are ready
kubectl get nodes
# All nodes should have STATUS=Ready

# Check system pods
kubectl get pods -n kube-system -o wide
# All pods should be Running

# Verify API server is responding
kubectl api-resources | wc -l
```

#### Phase 3: Module Upgrade

**3.1 Update Distribution Modules**

```bash
# Create updated furyctl configuration with v1.35 modules
cat > furyctl-1.35-update.yaml <<EOF
apiVersion: kfd.sighup.io/v1alpha2
kind: OnPremises
metadata:
  name: my-cluster
spec:
  distributionVersion: v1.35.0

  # Ensure modules meet minimum versions
  # See compatibility matrix
  modules:
    auth: v0.6.0
    logging: v5.2.0
    networking: v3.0.0
    monitoring: v4.0.1
    # ... other modules
EOF

# Apply module updates
furyctl apply --config furyctl-1.35-update.yaml
```

**3.2 Verify Module Upgrades**

```bash
# Check distribution was applied
kubectl get pods -A | grep -E "auth|logging|monitoring"

# Verify pods are running (not CrashLoopBackOff)
kubectl get pods -A -o wide

# Check logs for errors
kubectl logs -n auth --all-containers=true -l app=dex --tail=50
kubectl logs -n logging --all-containers=true --tail=50
```

---

### EKS Clusters

#### Phase 1: Preparation

**1.1 Update Configuration**

```yaml
# Update your EKS furyctl configuration
apiVersion: kfd.sighup.io/v1alpha2
kind: EKSCluster
metadata:
  name: my-eks-cluster
spec:
  distributionVersion: v1.35.0  # Change from v1.34.0

  # Rest of configuration remains the same
  region: us-west-2
  kubernetes:
    nodeAllowedSshPublicKey: ssh-ed25519 AAAA...
    # ... rest of config
```

**1.2 Run Preflight Checks**

```bash
# Validate configuration
furyctl create --config eks-furyctl.yaml --dry-run

# Should complete successfully with no breaking changes errors
```

**1.3 Backup Cluster Configuration**

```bash
# Update kubeconfig
aws eks update-kubeconfig \
  --region us-west-2 \
  --name my-eks-cluster

# Backup current state
kubectl get all -A -o yaml > eks-state.1.34.yaml

# Export CloudFormation templates (if using CFN)
aws cloudformation describe-stacks \
  --stack-name eks-cluster-stack \
  > eks-stack-backup.json
```

#### Phase 2: Cluster Upgrade

**2.1 Upgrade EKS Control Plane**

```bash
# Update EKS cluster to 1.35
aws eks update-cluster-version \
  --name my-eks-cluster \
  --kubernetes-version 1.35

# Monitor upgrade progress
aws eks describe-cluster \
  --name my-eks-cluster \
  --query 'cluster.status'
# Watch for: ACTIVE (after UPDATE_IN_PROGRESS)

# Wait for control plane upgrade to complete (15-45 minutes)
```

**2.2 Update Node Groups**

```bash
# Get list of node groups
aws eks list-nodegroups --cluster-name my-eks-cluster

# Update each node group (one at a time)
NODE_GROUP="worker-group-1"

aws eks update-nodegroup-version \
  --cluster-name my-eks-cluster \
  --nodegroup-name $NODE_GROUP \
  --kubernetes-version 1.35

# Monitor upgrade
watch -n 10 "aws eks describe-nodegroup \
  --cluster-name my-eks-cluster \
  --nodegroup-name $NODE_GROUP \
  --query 'nodegroup.status'"
```

**2.3 Verify Upgrade**

```bash
# Check cluster version
kubectl version --short

# Verify all nodes are ready
kubectl get nodes
# All nodes should show v1.35.x

# Check system pods
kubectl get pods -n kube-system
```

#### Phase 3: Module Upgrade (Optional)

```bash
# If using EKS with KFDDistribution modules
furyctl apply --config eks-furyctl-1.35.yaml

# Verify modules deployed
kubectl get pods -A | head -20
```

---

### KFDDistribution (Existing Clusters)

#### For Distribution Deployment on 1.35 Clusters

**1.1 Verify Cluster is Running 1.35**

```bash
# Check Kubernetes version
kubectl version --short
# Should show: v1.35.x

# Run validation script
./scripts/validate-k8s-1.35.sh
# All checks should pass
```

**1.2 Update Distribution Configuration**

```yaml
apiVersion: kfd.sighup.io/v1alpha2
kind: KFDDistribution
metadata:
  name: fury-dist
spec:
  distributionVersion: v1.35.0

  modules:
    auth:
      namespace: dex
    logging:
      namespace: logging
    # Ensure all modules meet minimum versions
```

**1.3 Deploy/Update Distribution**

```bash
# Apply distribution
furyctl apply --config distribution-1.35.yaml

# Verify modules deployed
kubectl get pods -A | grep -E "auth|logging|monitoring"
```

---

## Handling Breaking Changes

### Breaking Change 1: WebSocket RBAC Validation (auth module)

**Issue:** Authentication required for WebSocket connections

**Symptoms:**
- WebSocket connections fail with 403 Forbidden
- Applications using WebSockets cannot authenticate

**Resolution:**

```bash
# Check current RBAC policies
kubectl get rolebindings -A -o wide | grep -E "websocket|ws"

# Update policies to allow WebSocket access
# Example for Kubernetes Dashboard
kubectl create clusterrolebinding websocket-rbac \
  --clusterrole=view \
  --serviceaccount=kubernetes-dashboard:kubernetes-dashboard

# Verify WebSocket connectivity
kubectl proxy &
# Visit: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/...
```

### Breaking Change 2: New Audit Log Format (logging module)

**Issue:** Audit logs have new fields in Kubernetes 1.35

**Symptoms:**
- Log parsing fails or warnings appear
- Elasticsearch ingestion errors

**Resolution:**

```bash
# Check current audit logs
kubectl logs -n kube-system -l component=kube-apiserver --tail=100

# Update log parsing rules (example for Fluentd)
kubectl patch configmap fluentd-config -n logging --type merge \
  -p '{"data":{"parsers.conf":"<parse>\n  @type json\n  # Updated for K8s 1.35 fields\n</parse>"}}'

# Verify logs are being processed
kubectl logs -n logging -l app=fluentd
```

### Breaking Change 3: Prometheus Metrics Renamed (monitoring module)

**Issue:** Some metric names changed in Kubernetes 1.35

**Symptoms:**
- Grafana dashboards show "No data"
- Prometheus alerts fail to evaluate

**Resolution:**

```bash
# List available metrics
kubectl port-forward -n monitoring svc/prometheus 9090:9090 &

# Visit: http://localhost:9090/api/v1/query?query=up

# Update Grafana dashboards
# In Grafana UI: Dashboard → Edit → Update metric names

# Example metric renaming:
# Old: kubelet_volume_stats_available_bytes
# New: kubelet_volume_stats_capacity_bytes

# Verify metrics in Prometheus
# Visit metrics page in Prometheus UI
```

### Breaking Change 4: Network Policy Enforcement (networking module)

**Issue:** Network policies are more strictly enforced

**Symptoms:**
- Unexpected connectivity issues between pods
- Cross-namespace communication blocked

**Resolution:**

```bash
# Check network policies
kubectl get networkpolicies -A

# Test connectivity between pods
POD_A="pod-a"
POD_B="pod-b"
kubectl exec -it $POD_A -- nc -zv $POD_B 8080

# Add explicit network policies if needed
cat > allow-traffic.yaml <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-cross-ns
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector: {}
  egress:
  - to:
    - namespaceSelector: {}
EOF

kubectl apply -f allow-traffic.yaml
```

---

## Rollback Procedure

If upgrade fails or critical issues arise:

### OnPremises Rollback

```bash
# 1. Stop control plane
sudo systemctl stop kube-apiserver kube-controller-manager kube-scheduler

# 2. Restore etcd backup
sudo systemctl stop etcd
sudo rm -rf /var/lib/etcd
sudo cp -r /var/lib/etcd.backup.1.34 /var/lib/etcd
sudo systemctl start etcd

# 3. Revert kubeadm
sudo kubeadm reset
sudo kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml

# 4. Verify rollback
kubectl version --short
# Should show v1.34.x
```

### EKS Rollback

```bash
# 1. Update cluster back to 1.34
aws eks update-cluster-version \
  --name my-eks-cluster \
  --kubernetes-version 1.34

# 2. Update node groups back to 1.34
aws eks update-nodegroup-version \
  --cluster-name my-eks-cluster \
  --nodegroup-name worker-group-1 \
  --kubernetes-version 1.34

# 3. Verify rollback
kubectl version --short
```

### KFDDistribution Rollback

```bash
# 1. Revert distribution configuration
kubectl apply -f distribution-1.34.yaml

# 2. Verify modules rolled back
kubectl get pods -A | grep -E "auth|logging|monitoring"
```

---

## Post-Upgrade Validation

### Checklist

- [ ] Kubernetes version is 1.35.x
- [ ] All nodes report Ready status
- [ ] All system pods are Running
- [ ] All application pods are Running
- [ ] Persistent volumes are Bound
- [ ] No pending pods
- [ ] Monitoring and logging are functional
- [ ] Network connectivity works as expected
- [ ] Authentication and RBAC work correctly
- [ ] Custom applications function normally

### Testing Commands

```bash
# Comprehensive validation
./scripts/validate-k8s-1.35.sh

# Application testing (example)
kubectl create deployment test --image=nginx
kubectl wait --for=condition=available \
  --timeout=300s deployment/test
kubectl logs deployment/test

# Clean up test deployment
kubectl delete deployment test
```

---

## Troubleshooting

### Issue: Nodes Not Ready After Upgrade

```bash
# Check node status
kubectl get nodes -o wide

# Check node logs
kubectl describe node <node-name>

# Check kubelet logs
ssh user@<node-ip>
sudo journalctl -u kubelet -n 50 -f

# Remedy: Restart kubelet
sudo systemctl restart kubelet
```

### Issue: Pods Not Starting

```bash
# Check pod status
kubectl describe pod <pod-name> -n <namespace>

# Check logs
kubectl logs <pod-name> -n <namespace>

# Common causes:
# 1. Image compatibility (update image)
# 2. Resource limits (increase limits)
# 3. RBAC permissions (fix RBAC policy)
```

### Issue: etcd Connection Errors

```bash
# Check etcd status
kubectl get endpoints -n kube-system etcd

# Verify etcd is healthy
kubectl exec -n kube-system -it <etcd-pod> \
  -- etcdctl endpoint health

# If etcd is corrupt, restore from backup
sudo systemctl stop etcd
sudo cp -r /var/lib/etcd.backup.1.34 /var/lib/etcd
sudo systemctl start etcd
```

---

## Performance Tuning Post-Upgrade

After successful upgrade, consider:

1. **Update Resource Requests/Limits**
   - Monitor actual usage for 1 week
   - Adjust requests/limits based on 1.35 overhead (3-8% more memory)

2. **Optimize RBAC Policies**
   - Review audit logs for denied requests
   - Add necessary permissions

3. **Update Networking**
   - Optimize network policies for improved enforcement
   - Consider QoS policies

---

## Upgrade Timeline Estimate

| Cluster Type | Control Plane | Node Groups | Modules | Validation | Total |
|--------------|---------------|-------------|---------|-----------|-------|
| OnPremises 3 nodes | 10-15 min | 20-30 min | 5-10 min | 5 min | 40-60 min |
| EKS 10 nodes | 15-45 min | 30-60 min | 5-10 min | 5 min | 55-120 min |
| KFDDistribution | 5-10 min | N/A | 5-10 min | 5 min | 15-25 min |

---

## Support and Resources

- **Issues During Upgrade:** See troubleshooting section
- **Breaking Changes Details:** [K8S-1.35-BREAKING-CHANGES.md](./K8S-1.35-BREAKING-CHANGES.md)
- **Testing Guide:** [K8S-1.35-TESTING.md](./K8S-1.35-TESTING.md)
- **Compatibility Matrix:** [COMPATIBILITY-MATRIX.md](./COMPATIBILITY-MATRIX.md)
- **Official K8s Upgrade Guide:** https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

---

## Upgrade Completion

After successful upgrade and validation:

1. ✓ Document upgrade in change management
2. ✓ Update cluster documentation
3. ✓ Notify stakeholders of completion
4. ✓ Schedule follow-up monitoring (1 week)
5. ✓ Plan for 1.36 upgrade in ~12 months

Congratulations! Your cluster is now running Kubernetes 1.35.x
