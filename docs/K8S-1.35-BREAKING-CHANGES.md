# Kubernetes 1.35 Breaking Changes

## Overview

This document describes breaking changes in Kubernetes 1.35 that affect SIGHUP Distribution deployments and provides migration guidance for each change.

**Last Updated:** January 2025
**Kubernetes 1.35 Release:** January 2025
**furyctl Support:** v0.34.0+
**Distribution Support:** v1.35.0+

---

## Critical Changes (Action Required)

### 1. WebSocket RBAC Validation (auth module)

**Component:** Kubernetes API Server
**Severity:** ðŸ”´ **CRITICAL** - May break WebSocket connections
**Affected Versions:** 1.35.0+
**Module Impact:** auth v0.6.0+

#### What Changed

Kubernetes 1.35 introduces stricter RBAC validation for WebSocket connections. Previously, WebSocket connections could bypass certain RBAC checks. Now, WebSocket endpoints require explicit RBAC permissions.

#### Impact

- WebSocket connections to the API server require proper RBAC permissions
- Custom applications using WebSocket connections may fail authentication
- Dashboard and monitoring tools using WebSockets may need updated permissions

#### Examples Affected

- Kubernetes Dashboard (uses WebSockets)
- Custom applications with WebSocket endpoints
- Debugging tools that use WebSocket proxies
- Real-time log streaming tools

#### Detection

```bash
# Check if WebSocket RBAC is blocking connections
kubectl logs -n kube-system -l component=kube-apiserver | grep -i websocket

# Expected error messages:
# "WebSocket authentication failed"
# "RBAC: user cannot access websocket endpoint"
# "Connection refused on WebSocket upgrade"
```

#### Migration Path

**Step 1: Identify WebSocket-using Components**

```bash
# Search for WebSocket clients in your cluster
grep -r "websocket\|ws://" /path/to/deployments

# Check for tools using WebSocket:
kubectl get pods -A -o yaml | grep -i websocket
```

**Step 2: Update RBAC Policies**

```bash
# Create or update ClusterRoleBinding for WebSocket access
cat > websocket-rbac.yaml <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: websocket-access
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "watch", "list"]
- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create", "get"]
EOF

kubectl apply -f websocket-rbac.yaml

# Bind role to users/service accounts
kubectl create clusterrolebinding dashboard-websocket \
  --clusterrole=websocket-access \
  --serviceaccount=kubernetes-dashboard:kubernetes-dashboard
```

**Step 3: Test WebSocket Connectivity**

```bash
# Test WebSocket connection
kubectl proxy &
# Visit: http://localhost:8001/api/v1/namespaces/kube-system/pods

# Monitor for WebSocket errors
kubectl logs -n kube-system deployment/kube-apiserver | tail -100
```

**Step 4: Verify Dashboard Access**

```bash
# Port forward to dashboard
kubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:443 &

# Visit: https://localhost:8443
# Verify real-time pod logs work (uses WebSocket)
```

#### Troubleshooting

If WebSocket connections still fail after RBAC update:

1. **Check user identity:**
   ```bash
   kubectl auth can-i get pods --as=<user>
   kubectl auth can-i get pods/log --as=<user>
   ```

2. **Enable API audit logging:**
   ```bash
   kubectl logs -n kube-system -l component=kube-apiserver \
     | grep -i "websocket\|upgrade"
   ```

3. **Review auth module version:**
   ```bash
   helm list -n dex
   # Should show auth v0.6.0 or later
   ```

---

### 2. Containerd 2.0+ Requirement (OnPremises only)

**Component:** Container Runtime
**Severity:** ðŸ”´ **CRITICAL** - Cannot start clusters without this
**Affected Versions:** 1.35.0+
**Affected Cluster Types:** OnPremises only

#### What Changed

Kubernetes 1.35 requires containerd 2.0 or later. containerd 1.x has reached end-of-life and is not compatible with 1.35 API changes.

#### Impact

- OnPremises clusters **cannot** be created/upgraded with containerd 1.x
- **Blocker:** Preflight check will fail with clear error message
- Cluster creation will not proceed without containerd 2.0+

#### Detection

```bash
# Check containerd version on nodes
containerd --version
# Output should be: containerd github.com/containerd/containerd v2.0.x

# If version is 1.x:
# containerd github.com/containerd/containerd v1.7.x
# This will be rejected during preflight checks
```

#### Migration Path

**Step 1: Plan Upgrade Window**

```bash
# Determine upgrade time (requires node restart)
# Plan 30-60 minutes per node
# Upgrade master nodes first, then workers
```

**Step 2: Backup Node Configuration**

```bash
# On each node to be upgraded
sudo cp -r /etc/containerd /etc/containerd.backup.v1
sudo cp -r /var/lib/containerd /var/lib/containerd.backup.v1
```

**Step 3: Upgrade containerd**

**For Ubuntu/Debian:**
```bash
# Drain node first
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# Update package list
sudo apt-get update

# Install containerd 2.0
sudo apt-get install -y containerd.io=2.0.0 # or latest stable 2.x

# Restart containerd
sudo systemctl restart containerd

# Verify installation
containerd --version
# Output: containerd github.com/containerd/containerd v2.0.x

# Uncordon node
kubectl uncordon <node-name>
```

**For RHEL/CentOS:**
```bash
# Drain node first
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# Install containerd 2.0
sudo yum update -y containerd
# Or specify version:
sudo yum install -y containerd-2.0.0

# Restart containerd
sudo systemctl restart containerd

# Verify installation
containerd --version

# Uncordon node
kubectl uncordon <node-name>
```

**Step 4: Verify Upgrade**

```bash
# Check all nodes have containerd 2.0+
kubectl get nodes -o wide
# Nodes should show Ready status

# Check container runtime status
kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.containerRuntimeVersion}'
# Output should show: containerd://2.0.x

# Verify pods are starting correctly
kubectl get pods -A --field-selector=status.phase=Pending
# Should return no pending pods
```

#### Troubleshooting

**Issue: Nodes not ready after containerd upgrade**

```bash
# Check containerd service status
sudo systemctl status containerd

# Check containerd logs
sudo journalctl -u containerd -n 50

# If fails to start:
# 1. Check cgroup version (must be v2)
stat -fc %T /sys/fs/cgroup/

# 2. Restore backup if needed
sudo systemctl stop containerd
sudo rm -rf /etc/containerd /var/lib/containerd
sudo cp -r /etc/containerd.backup.v1 /etc/containerd
sudo cp -r /var/lib/containerd.backup.v1 /var/lib/containerd
sudo systemctl start containerd
```

**Issue: Container images fail to pull**

```bash
# containerd may be using different image registry paths
# Update mirror configuration in /etc/containerd/config.toml

# Restart containerd with new config
sudo systemctl restart containerd

# Verify image pull works
sudo ctr images pull docker.io/library/nginx:latest
```

---

### 3. cgroup v2 Requirement (OnPremises only)

**Component:** Linux Kernel
**Severity:** ðŸ”´ **CRITICAL** - Cannot run clusters without this
**Affected Versions:** 1.35.0+
**Affected Cluster Types:** OnPremises only

#### What Changed

Kubernetes 1.35 requires cgroup v2 (unified hierarchy). cgroup v1 is no longer supported due to resource management limitations.

#### Impact

- **Blocker:** Nodes with cgroup v1 cannot join 1.35 cluster
- **OS requirement:** Ubuntu 22.04+, RHEL/CentOS 9+, Debian 12+
- **Migration impact:** May require full OS upgrade for older systems

#### Detection

```bash
# Check current cgroup version on nodes
stat -fc %T /sys/fs/cgroup/

# Output should be: cgroup2fs (v2)
# If output is: tmpfs (indicates v1)
# Then cgroup v1 is active - UPGRADE REQUIRED
```

#### Migration Path

**Step 1: Identify Affected Nodes**

```bash
# On each node:
stat -fc %T /sys/fs/cgroup/

# Create list of nodes with v1:
for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
  echo -n "$node: "
  ssh user@$node "stat -fc %T /sys/fs/cgroup/ 2>/dev/null || echo 'cgroup-v1'"
done
```

**Step 2: Plan OS Upgrade**

cgroup v2 requires modern OS. Supported versions:

```
Ubuntu:
  - 22.04 LTS (or newer)
  - 24.04 LTS (recommended)

RHEL/CentOS:
  - 9.0 or newer
  - 8.x NOT SUPPORTED

Debian:
  - 12 (or newer)
  - 11 NOT SUPPORTED

AlmaLinux:
  - 9.0 or newer
```

**Step 3: In-place Upgrade (If possible)**

```bash
# For Ubuntu 20.04 â†’ 22.04
sudo do-release-upgrade
# or
sudo sed -i 's/focal/jammy/g' /etc/apt/sources.list
sudo apt-get update
sudo apt-get upgrade -y
sudo reboot

# Verify after reboot:
stat -fc %T /sys/fs/cgroup/
# Should output: cgroup2fs
```

**Step 4: OS Full Reinstall (If upgrade fails)**

```bash
# For complete OS replacement:
# 1. Backup node data
sudo tar -czf /backup/node-data.tar.gz /var/lib/

# 2. Rebuild OS with cgroup v2 support
# Use official installation media for target OS

# 3. Restore data if needed
sudo tar -xzf /backup/node-data.tar.gz -C /

# 4. Rejoin cluster
sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>
```

#### Troubleshooting

**Issue: Kernel is old and doesn't support cgroup v2**

```bash
# Check kernel version
uname -a

# Minimum kernel versions for cgroup v2:
# Linux 5.2+ (Ubuntu 20.04 has 5.15+, which supports cgroup v2)

# If kernel too old, upgrade kernel or OS
```

**Issue: cgroup v1 enabled in GRUB**

```bash
# Check GRUB configuration
cat /proc/cmdline | grep cgroup

# If shows "cgroup_disable=memory" or similar:
# Edit /etc/default/grub and remove cgroup flags

sudo nano /etc/default/grub
# Remove any "cgroup_disable" entries

sudo update-grub
sudo reboot

# Verify:
stat -fc %T /sys/fs/cgroup/
```

---

## Important Changes (Review Required)

### 4. IPVS Deprecation (Warning - Not Blocking)

**Component:** kube-proxy
**Severity:** ðŸŸ¡ **IMPORTANT** - Warn now, will block in 1.36
**Affected Versions:** 1.35.0+ (warning), 1.36.0+ (removal)
**Affected Cluster Types:** All types

#### What Changed

IPVS mode for kube-proxy is deprecated. Support will be removed in Kubernetes 1.36. Migration to nftables is recommended.

#### Impact

- **1.35:** Warning logged, functionality still works
- **1.36:** IPVS support removed, clusters must use iptables or nftables
- **Recommendation:** Migrate before 1.36 release

#### Detection

```bash
# Check kube-proxy mode
kubectl get ds -n kube-system kube-proxy -o yaml | grep -A5 "command:"

# Look for: --proxy-mode=ipvs
# If found, migration required
```

#### Migration Path

**Step 1: Verify Current Mode**

```bash
# Check which mode is active
kubectl get configmap -n kube-system kube-proxy -o yaml | grep mode

# Expected values:
# - iptables (default, no action needed)
# - ipvs (migration required)
```

**Step 2: Switch to nftables**

```bash
# Update kube-proxy configuration
kubectl patch configmap kube-proxy -n kube-system --type merge \
  -p '{"data":{"config.conf":"mode: nftables"}}'

# Restart kube-proxy daemonset
kubectl rollout restart ds/kube-proxy -n kube-system

# Verify mode switched
kubectl logs -n kube-system -l k8s-app=kube-proxy | grep -i "mode"
# Should show: Using nftables mode
```

**Step 3: Test Connectivity**

```bash
# Test service access still works
kubectl run test-pod --image=curlimages/curl -it --rm -- \
  curl http://kubernetes.default.svc.cluster.local

# Test external connectivity
kubectl run test-pod --image=curlimages/curl -it --rm -- \
  curl http://example.com
```

---

### 5. Pod Disruption Budget v2

**Component:** Pod Disruption Budgets
**Severity:** ðŸŸ¢ **LOW** - Informational change
**Affected Versions:** 1.35.0+
**Affected Cluster Types:** All types

#### What Changed

PodDisruptionBudgetV2 is now GA (generally available). New version supports more flexibility in disruption policies.

#### Impact

- **Existing PDBs:** Continue to work with v1 API
- **New deployments:** Should use v2 API
- **Migration:** Optional but recommended for new workloads

#### Migration Path

```bash
# Old PDB (v1)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: my-app

# New PDB (v2) - More flexible
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 1
  unhealthyPodEvictionPolicy: IfHealthyBudget
  selector:
    matchLabels:
      app: my-app
```

---

### 6. Kubernetes API Deprecations

**Component:** Various APIs
**Severity:** ðŸŸ¡ **IMPORTANT** - Plan removal
**Affected Versions:** 1.35.0+ (warning), 1.37+ (removal)

#### Deprecated APIs

| API | Version | Replacement | Timeline |
|-----|---------|-------------|----------|
| apps/v1beta1 | Deprecated | apps/v1 | Remove 1.37 |
| extensions/v1beta1 | Deprecated | batch/v1 | Remove 1.37 |
| policy/v1beta1 | Deprecated | policy/v1 | Remove 1.37 |

#### Migration Path

```bash
# Identify deprecated APIs in use
kubectl api-resources -o wide

# Update manifests to use v1 APIs
sed -i 's/apps\/v1beta1/apps\/v1/g' deployment.yaml
sed -i 's/policy\/v1beta1/policy\/v1/g' pdb.yaml

# Test updated manifests
kubectl apply -f deployment.yaml --dry-run=client
```

---

## Module-Specific Breaking Changes

### Module: auth (v0.5.0 â†’ v0.6.0)

**Changes:**
- WebSocket RBAC validation (see section above)
- OAuth2 token endpoint changes
- OIDC provider configuration requirements

**User Action:**
- [ ] Review OIDC provider configuration
- [ ] Update RBAC policies for WebSocket
- [ ] Test authentication flows

### Module: logging (v5.1.0 â†’ v5.2.0)

**Changes:**
- New audit log fields for Kubernetes 1.35 events
- Elasticsearch mapping updates
- Log retention policy changes

**User Action:**
- [ ] Update Elasticsearch index mappings
- [ ] Update log parsing rules
- [ ] Reconfigure retention policies
- [ ] Verify log ingestion pipeline

**Update ES Index Mapping:**
```bash
# Connect to Elasticsearch
curl -X PUT "localhost:9200/kubernetes-audit-1.35" \
  -H 'Content-Type: application/json' -d'{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1
  },
  "mappings": {
    "properties": {
      "@timestamp": { "type": "date" },
      "level": { "type": "keyword" },
      "user.username": { "type": "keyword" },
      "sourceIPs": { "type": "ip" }
    }
  }
}'
```

### Module: monitoring (v4.0.0 â†’ v4.0.1)

**Changes:**
- Prometheus metric names updated
- Grafana dashboard version bumped
- Alerting rules require updates

**User Action:**
- [ ] Update Grafana dashboards
- [ ] Update Prometheus alert rules
- [ ] Verify metric ingestion
- [ ] Test alerting notifications

**Update Prometheus Alerts:**
```bash
# Get current alerts
kubectl get rules -n monitoring

# Update alert definitions
kubectl patch prometheusrule monitoring-rules -n monitoring --type merge \
  -p '{"spec":{"groups":[{"name":"k8s-1.35","rules":[...]}]}}'

# Verify alerts loaded
kubectl logs -n monitoring prometheus-0 | grep -i "alert"
```

### Module: networking (v3.0.0)

**Changes:**
- CNI improvements and optimizations
- Network policy enforcement stricter
- IP pool management enhancements

**User Action:**
- [ ] Review existing network policies
- [ ] Test cross-namespace connectivity
- [ ] Verify DNS resolution
- [ ] Check service access

---

## Compatibility Checklist

Before upgrading to 1.35, verify:

- [ ] No applications using IPVS mode
- [ ] All nodes have cgroup v2 or upgradeplan in place
- [ ] containerd 2.0+ installed on OnPremises nodes
- [ ] Module versions meet minimum requirements
- [ ] RBAC policies reviewed for WebSocket support
- [ ] Elasticsearch mapping updated for new audit logs
- [ ] Grafana dashboards compatible with new metrics
- [ ] Network policies tested
- [ ] All deprecated APIs replaced in manifests
- [ ] Backup of etcd and persistent volumes
- [ ] Maintenance window scheduled
- [ ] Rollback plan documented

---

## Support and Escalation

### Getting Help

If you encounter breaking change issues:

1. **Check relevant documentation:**
   - Module-specific release notes
   - [K8S-1.35-UPGRADE-GUIDE.md](./K8S-1.35-UPGRADE-GUIDE.md)
   - [K8S-1.35-TESTING.md](./K8S-1.35-TESTING.md)

2. **Run validation script:**
   ```bash
   ./scripts/validate-k8s-1.35.sh
   ```

3. **Check logs for details:**
   ```bash
   furyctl logs --all
   ```

4. **Report issue with:**
   - Exact error message
   - Cluster type (OnPremises/EKS/KFDDistribution)
   - Current and target versions
   - Steps to reproduce

---

## Summary

| Change | Severity | Blocking | Action Required |
|--------|----------|----------|-----------------|
| WebSocket RBAC | ðŸ”´ Critical | Partial | Update RBAC immediately |
| containerd 2.0+ | ðŸ”´ Critical | Yes | Upgrade before cluster create |
| cgroup v2 | ðŸ”´ Critical | Yes | Upgrade OS before upgrade |
| IPVS deprecation | ðŸŸ¡ Important | No | Plan migration for 1.36 |
| New audit logs | ðŸŸ¡ Important | No | Update logging pipeline |
| Metric renames | ðŸŸ¡ Important | No | Update monitoring |
| API deprecations | ðŸŸ¡ Important | No | Update manifests |

**Recommended Order:**
1. Verify cgroup v2 and containerd 2.0+
2. Update RBAC policies
3. Test in staging environment
4. Perform upgrade
5. Update monitoring/logging configurations
6. Plan IPVS migration for next quarter

---

## Additional Resources

- [Kubernetes 1.35 Release Notes](https://kubernetes.io/blog/2025/01/kubernetes-1-35-release/)
- [containerd v2.0 Release Notes](https://github.com/containerd/containerd/releases/tag/v2.0.0)
- [SIGHUP Fury Distribution](https://docs.fury.sighup.io)

---

**Document Version:** 1.0
**Last Updated:** January 2025
**Status:** âœ“ Complete for 1.35.0 release
